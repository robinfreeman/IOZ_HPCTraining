{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ade3d-dbee-43b6-b6e5-fe8100581dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# GPU Image Classification with ResNet-50\n",
    "# --------------------------------------------------------------\n",
    "# Fine-tuning a pretrained ResNet-50 model for bird species\n",
    "# classification using PyTorch and TorchVision.\n",
    "# ==============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07bb31-9015-487f-bb1b-f0ccbec44d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Install dependencies (if needed) ----\n",
    "!pip install -q torch torchvision tqdm matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2995a7f-6f54-446c-9dbe-5e1fbaeae6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports ----\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7ae09-a57d-4870-a433-d14e6444cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1. GPU Configuration\n",
    "# ==============================================================\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# on apple M chips use:\n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "print(f\"Using device: {device}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12198ad-968d-47db-988a-089b92347436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c https://torch-cdn.mlverse.org/datasets/bird-species.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939faf14-afd0-4fbc-b6d5-adab846bdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -d data/bird-species bird-species.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb3e84-6eea-4f97-a22b-ad7120a16272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 2. Dataset and Transformations\n",
    "# ==============================================================\n",
    "\n",
    "train_path = \"data/bird-species/train/\"\n",
    "test_path = \"data/bird-species/test/\"\n",
    "\n",
    "# ---- Training transformations with augmentation ----\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---- Validation/Test transformations (no augmentation) ----\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6030-2288-451e-aa47-942423137b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3. Data Loading\n",
    "# ==============================================================\n",
    "\n",
    "# Load datasets\n",
    "full_train_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_path, transform=test_transform)\n",
    "\n",
    "# Split training data into train/validation (80/20 split)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=8, persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=64, shuffle=False, num_workers=8, persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "print(f\"Test images: {len(test_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c10e21-6fe7-493d-b07f-f11b55d357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4. Model Setup\n",
    "# ==============================================================\n",
    "\n",
    "# Load pretrained ResNet-152\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Replace the fully connected layer for our dataset\n",
    "num_classes = len(full_train_dataset.classes)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Regularisation\n",
    "    nn.Linear(512, num_classes),\n",
    ")\n",
    "\n",
    "# Move model to GPU (if available)\n",
    "model = model.to(device)\n",
    "\n",
    "# Unfreeze last two layers for fine-tuning\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer3\" in name or \"layer4\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9a99b-fde7-422c-b75b-7be657c6d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5. Optimiser, Loss Function, Scheduler\n",
    "# ==============================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f92995-e308-4286-81c1-5bd19e692078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 6. Training Loop\n",
    "# ==============================================================\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Outer progress bar to track epoch progress\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training phase\n",
    "    # ------------------------------\n",
    "    for images, labels in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"\n",
    "    ):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    # Compute mean training loss\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Validation phase\n",
    "    # ------------------------------\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(\n",
    "            val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"\n",
    "        ):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    # Print summary\n",
    "    print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "            f\"Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Step learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f87d02-77c9-46f5-8085-19fab67ec5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 7. Post-Training: Plot and Save Learning Curves\n",
    "# ==============================================================\n",
    "\n",
    "# Create results folder if it doesn’t exist\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "plot_path_png = \"results/training_validation_loss.png\"\n",
    "plot_path_pdf = \"results/training_validation_loss.pdf\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(val_losses, label=\"Validation Loss\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(plot_path_png, dpi=300)\n",
    "plt.savefig(plot_path_pdf)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Saved training loss plot to:\\n  • {plot_path_png}\\n  • {plot_path_pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7710f-7ed5-433c-a3b9-c84e9bca36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 8. GPU Benchmark Summary\n",
    "# --------------------------------------------------------------\n",
    "# Display GPU usage and performance information\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" GPU BENCHMARK SUMMARY \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    allocated_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    reserved_mem = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total memory: {total_mem:.2f} GB\")\n",
    "    print(f\"Allocated memory: {allocated_mem:.2f} GB\")\n",
    "    print(f\"Reserved memory: {reserved_mem:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU (no CUDA GPU detected).\")\n",
    "\n",
    "print(f\"\\nTraining time: {training_duration / 60:.2f} minutes\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07395f1",
   "metadata": {},
   "source": [
    "## 9. Evaluation & Visualizations\n",
    "\n",
    "We'll compute standard classification metrics on the test set, show a confusion matrix, and visualise a grid of example images with predicted vs true labels and confidences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551551c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Evaluation: metrics, confusion matrix, example predictions ----------\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_probs.append(probs)\n",
    "\n",
    "# Concatenate batches\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_probs = np.concatenate(all_probs)\n",
    "\n",
    "# Basic metrics\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Weighted Precision: {prec:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b67b2b-a922-45a2-85c9-f87ed76b1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification report (per-class)\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(\n",
    "    classification_report(\n",
    "        all_labels, all_preds, target_names=full_train_dataset.classes, digits=4\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca0807-427e-45ba-8ee5-a5adc16ec045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(20, 20))  # increase size\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=False,  # turn off annotation if it's unreadable\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=full_train_dataset.classes,\n",
    "    yticklabels=full_train_dataset.classes,\n",
    ")\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffd45a-4d54-4947-8a40-d2b3a9d5cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show example predictions: sample up to 16 images from the test set with predictions/confidences\n",
    "def imshow_tensor(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect a small batch of examples (first few batches) to visualise\n",
    "examples = []\n",
    "labels_list = []\n",
    "preds_list = []\n",
    "probs_list = []\n",
    "seen = 0\n",
    "max_examples = 16\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device))\n",
    "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        for i in range(images.size(0)):\n",
    "            if seen >= max_examples:\n",
    "                break\n",
    "            examples.append(images[i].cpu())\n",
    "            labels_list.append(labels[i].item())\n",
    "            preds_list.append(int(preds[i]))\n",
    "            probs_list.append(float(probs[i, preds[i]]))\n",
    "            seen += 1\n",
    "        if seen >= max_examples:\n",
    "            break\n",
    "\n",
    "# Plot example grid\n",
    "n = len(examples)\n",
    "cols = 4\n",
    "rows = int(np.ceil(n / cols))\n",
    "plt.figure(figsize=(cols * 3, rows * 3))\n",
    "for i in range(n):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    title = f\"True: {full_train_dataset.classes[labels_list[i]]}\\nPred: {full_train_dataset.classes[preds_list[i]]} ({probs_list[i]:.2f})\"\n",
    "    imshow_tensor(examples[i], title=title)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
